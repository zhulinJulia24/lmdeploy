name: daily_ete_test

on:
  workflow_dispatch:
    inputs:
      repo_org:
        required: false
        description: 'Tested repository organization name. Default is InternLM'
        type: string
        default: 'InternLM/lmdeploy'
      repo_ref:
        required: false
        description: 'Set branch or tag or commit id. Default is "main"'
        type: string
        default: 'main'
      backend:
        required: true
        description: 'Set backend testcase filter: turbomind or pytorch or turbomind, pytorch. Default is "["turbomind", "pytorch"]"'
        type: string
        default: "['turbomind', 'pytorch']"
      model:
        required: true
        description: 'Set testcase module filter: llm, vllm. Default contains all models'
        type: string
        default: "['llm','mllm']"
      function:
        required: true
        description: 'Set testcase function filter: chat, restful, pipeline. Default contains all functions'
        type: string
        default: '["pipeline", "restful", "chat"]'
      offline_mode:
        required: true
        description: 'Whether start a offline mode, if true, you should prepare code and whl package by yourself'
        type: boolean
        default: false
      regression_func:
        required: true
        description: 'regression functions'
        type: string
        default: "['restful']"
  schedule:
    - cron:  '00 14 * * 0-4'

env:
  HOST_PIP_CACHE_DIR: /nvme/github-actions/pip-cache
  HOST_LOCALTIME: /usr/share/zoneinfo/Asia/Shanghai
  OUTPUT_FOLDER: cuda12.8_dist_${{ github.run_id }}
  ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION: true
  REPORT_DIR: /nvme/qa_test_models/test-reports/${{ github.run_id }}
  COV_PARAM: --cov /opt/py3/lib/python3.10/site-packages/lmdeploy
  FAIL_CONFIG: ${{ github.event_name == 'schedule' && github.run_attempt != 1 && '--lf --lfnf none' || '--lf'}}
  TEST_CODE_PATH: /nvme/qa_test_models/test_pkg/lmdeploy/${{ github.run_id }}
  OFFLINE_CODE_PATH: /nvme/qa_test_models/offline_pkg/lmdeploy
  OFFLINE_REQUIREMENTS: /nvme/qa_test_models/offline_pkg/requirements.txt
  DEEPSEEK_VL: /nvme/qa_test_models/offline_pkg/DeepSeek-VL

jobs:
  linux-build:
    if: ${{!cancelled() && (github.event_name == 'schedule' || !inputs.offline_mode)}}
    strategy:
      matrix:
        pyver: [py310]
    runs-on: ubuntu-latest
    env:
      PYTHON_VERSION: ${{ matrix.pyver }}
      PLAT_NAME: manylinux2014_x86_64
      DOCKER_TAG: cuda12.8
    steps:
      - name: Free disk space
        uses: jlumbroso/free-disk-space@main
        with:
          # This might remove tools that are actually needed, if set to "true" but frees about 6 GB
          tool-cache: false
          docker-images: false
          # All of these default to true, but feel free to set to "false" if necessary for your workflow
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          swap-storage: false
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          repository: ${{ github.event.inputs.repo_org || 'InternLM/lmdeploy' }}
          ref: ${{github.event.inputs.repo_ref || 'main'}}
      - name: Build
        run: |
          echo ${PYTHON_VERSION}
          echo ${PLAT_NAME}
          echo ${DOCKER_TAG}
          echo ${OUTPUT_FOLDER}
          echo ${GITHUB_RUN_ID}
          # remove -it
          sed -i 's/docker run --rm -it/docker run --rm/g' builder/manywheel/build_wheel.sh
          bash builder/manywheel/build_wheel.sh ${PYTHON_VERSION} ${PLAT_NAME} ${DOCKER_TAG} ${OUTPUT_FOLDER}
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          if-no-files-found: error
          path: builder/manywheel/${{ env.OUTPUT_FOLDER }}
          retention-days: 1
          name: my-artifact-${{ github.run_id }}-${{ matrix.pyver }}

  test_restful:
    if: ${{!cancelled() && (github.event_name == 'schedule' || contains(fromJSON(github.event.inputs.regression_func), 'restful'))}}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        backend: ${{ fromJSON(inputs.backend || '["turbomind", "pytorch"]')}}
        model: ['internlm/Intern-S1', 'internlm/internlm2_5-20b-chat', 'internlm/internlm2_5-20b', 'Qwen/Qwen3-32B', 'OpenGVLab/InternVL3-38B']
        scene_type: ['base', 'logprob', 'expert']
        include:
          - tp: 2
            model: internlm/internlm2_5-20b-chat
            case_info: ['chat_completions', 'generate']
            scene_type: base
          - tp: 2
            model: internlm/internlm2_5-20b
            case_info: ['completions']
            scene_type: base
          - tp: 8
            model: internlm/Intern-S1
            case_info: ['chat_completions', 'generate']
            scene_type: base
          - tp: 2
            model: Qwen/Qwen3-32B
            case_info: ['chat_completions', 'generate']
            scene_type: base
          - tp: 2
            model: OpenGVLab/InternVL3-38B
            case_info: ['chat_completions', 'generate']
            scene_type: base
          - tp: 2
            model: Qwen/Qwen3-32B
            case_info: ['chat_completions', 'generate_logprob']
            scene_type: logprob
            mark: 'not expert'
            extra: '--logprobs-mode raw_logprobs'
          - tp: 2
            model: OpenGVLab/InternVL3-38B
            case_info: ['chat_completions', 'generate_logprob']
            scene_type: logprob
            mark: 'not expert'
            extra: '--logprobs-mode raw_logprobs'
          - tp: 2
            model: Qwen/Qwen3-32B
            case_info: ['chat_completions', 'generate_expert']
            scene_type: expert
            mark: 'not logprob'
            extra: '--enable-return-routed-experts'
          - tp: 2
            model: OpenGVLab/InternVL3-38B
            case_info: ['chat_completions', 'generate_expert']
            scene_type: expert
            mark: 'not logprob'
            extra: '--enable-return-routed-experts'
            backend: pytorch
    timeout-minutes: 60
    steps:
      - name: echo
        run: |
          echo "${{matrix.tp}}, ${{matrix.model}}, ${{matrix.case_info}}, ${{matrix.mark}}, ${{matrix.extra}}"
