name: daily_ete_test

on:
  workflow_dispatch:
    inputs:
      repo_org:
        required: false
        description: 'Tested repository organization name. Default is InternLM'
        type: string
        default: 'InternLM/lmdeploy'
      repo_ref:
        required: false
        description: 'Set branch or tag or commit id. Default is "main"'
        type: string
        default: 'main'
      backend:
        required: true
        description: 'Set backend testcase filter: turbomind or pytorch or turbomind, pytorch. Default is "["turbomind", "pytorch"]"'
        type: string
        default: "['turbomind', 'pytorch', 'turbomind-vl']"
      model:
        required: true
        description: 'Set testcase module filter: chat, restful, pipeline, quantization. Default contains all models'
        type: string
        default: "['quantization','convert','pipeline','restful','chat','local_case']"
      offline_mode:
        required: true
        description: 'Whether start a offline mode, if true, you should prepare code and whl package by yourself'
        type: boolean
        default: false
      dependency_pkgs:
        required: true
        description: 'Dependency packages, you can also set a specific version'
        type: string
        default: 'packaging transformers_stream_generator transformers==4.41.2 datasets matplotlib openai attrdict timm modelscope jmespath'
      tools_regression:
        required: true
        description: 'Whether start a tool regression'
        type: boolean
        default: true
      restful_regression:
        required: true
        description: 'Whether start a restful api regression'
        type: boolean
        default: true
      triton_regression:
        required: true
        description: 'Whether start a triton server api regression'
        type: boolean
        default: true
      pipeline_regression:
        required: true
        description: 'Whether start an interface pipeline regression'
        type: boolean
        default: true
  schedule:
    - cron:  '00 20 * * 0-4'

env:
  HOST_PIP_CACHE_DIR: /nvme/github-actions/pip-cache
  HOST_LOCALTIME: /usr/share/zoneinfo/Asia/Shanghai
  OUTPUT_FOLDER: cuda11.8_dist_${{ github.run_id }}
  TRITON_PTXAS_PATH: /usr/local/cuda/bin/ptxas


jobs:

  test_tools:
    if: ${{!cancelled() && (github.event_name == 'schedule' || inputs.tools_regression)}}
    runs-on: [self-hosted, linux-a100]
    timeout-minutes: 300
    env:
      REPORT_DIR: /nvme/qa_test_models/test-reports
      PYTHONPATH: /nvme/qa_test_models/offline_pkg/LLaVA
      MODELSCOPE_CACHE: /root/modelscope_hub
      MODELSCOPE_MODULES_CACHE: /root/modelscope_modules
    container:
      image: pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime
      options: "--gpus=all --ipc=host --user root -e PIP_CACHE_DIR=/root/.cache/pip --pull never"
      volumes:
        - /nvme/github-actions/pip-cache:/root/.cache/pip
        - /nvme/github-actions/packages:/root/packages
        - /nvme/github-actions/modelscope_hub:/root/modelscope_hub
        - /nvme/github-actions/modelscope_modules:/root/modelscope_modules
        - /nvme/github-actions/resources/lora:/root/lora
        - /nvme/qa_test_models:/nvme/qa_test_models
        - /nvme/qa_test_models/lmdeploy/autotest:/local_case
        - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
    steps:
      - name: Clone repository
        uses: actions/checkout@v2
        if: ${{github.event_name == 'schedule' || !inputs.offline_mode}}
        with:
          repository: ${{ github.event.inputs.repo_org || 'InternLM/lmdeploy' }}
          ref: ${{github.event.inputs.repo_ref || 'main'}}
      
